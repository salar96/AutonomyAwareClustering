%% Algorithm 4: Deep Clustering Algorithm (large N/ continuous X) - In-built ANN library
clear; clc; rng(1);

% ------------------ initialization -------------------
tic; close all; clc; 
idx = 4; [X,K,T_P,M,N] = data_RLClustering_ModelFree(idx);
[X,mu,sig] = zscore(X);

T = 0.0104; Tmin = 0.01; tau = 0.98;    % annealing parameters
eps = 0.1;                          % for epsilon greedy policy 
H_target = 250;                     % steps between target net updates
Sz_miniBatch = 512;                 % size of the minibatch for both nets and Y
buf_cap = 200000;                   % memory or replay capacity

MaxInnerTheta = 1500;               % policy update loop max iterations
MaxInnerY = 1500;                   % max iterations in Y update loop

alpha = 0.005;                      % learning rate for Y SGD
gamma = 0.001;                      % learning rate for policy SGD update

Px = (1/M)*ones(M,1);               % weight for each user data point
[idx_clust, Y] = kmeans(X,K);        % Starting with Y

P = 1/K*ones(M,K);                  % Policy 


idx_input = randi([1 length(X)],[Sz_miniBatch,1]);
init_trainInput = zeros(Sz_miniBatch,2*N);
init_trainOutput = zeros(Sz_miniBatch,1);
for i = 1 : Sz_miniBatch
    init_trainInput(i,:) = [X(idx_input(i),:) Y(idx_clust(idx_input(i)),:)];
    init_trainOutput(i) = norm(X(idx_input(i),:)-Y(idx_clust(idx_input(i),:))^2;
end
init_trainInput = init_trainInput';
init_trainOutput = init_trainOutput';

net = feedforwardnet([32 16], 'trainlm');
net = train(net, init_trainInput, init_trainOutput);

net_target = net;

memory.i = zeros(buf_cap,1); memory.j = zeros(buf_cap,1);
memory.k = zeros(buf_cap,1); memory.d = zeros(buf_cap,1);

buf_n = 0;                          % current size of the memory
circ_ptr = 1;                       % circular pointer

while T >= Tmin
    
    % ===== "policy" loop: update P given current Y =====
    for t = 1 : MaxInnerTheta
        i = randi(M); % sampling a user data point

        d_bar = zeros(K,1);
        for j = 1 : K
            %d_bar(j) = nn_forward(theta_target, X(i,:), Y(j,:));
            d_bar(j) = net_target([X(i,:),Y(j,:)]);
        end
        P(i,:) = (softmax_row(-(1/T)*d_bar))';

        % epsilon-greedy method for action selection
        if rand < eps
            j = randi(K);
        else
            [~,j] = max(P(i,:));
        end
        
        k = randsample(1:K, 1, true, T_P(:,j,i));   % sample the cluster k using T_P
        dist = norm(X(i,:) - Y(k,:))^2;            
        if buf_n < buf_cap
            loc = circ_ptr; buf_n = buf_n + 1;
        else
            loc = circ_ptr;
        end
        circ_ptr = circ_ptr + 1; 
        if circ_ptr > buf_cap
            circ_ptr = 1;
        end

        memory.i(loc) = i; memory.j(loc) = j; 
        memory.k(loc) = k; memory.d(loc) = dist;

        % train theta when enough data is available
        if buf_n >= Sz_miniBatch
            miniBatch = randi(buf_n, Sz_miniBatch, 1);   % uniformly sample
            miniBatchDataX = zeros(Sz_miniBatch,2*N);
            miniBatchDataY = zeros(Sz_miniBatch,1);
            loss = 0; gsum = nn_zeros_like(theta);  % what is this function?
            for b = 1 : Sz_miniBatch
                ii = memory.i(miniBatch(b));
                jj = memory.j(miniBatch(b));
                d_t = memory.d(miniBatch(b));
                miniBatchDataX(b) = [X(ii,:), Y(jj,:)];
                miniBatchDataY(b) = d_t(b);
            end
            net = trainnet(net, miniBatchDataX', miniBatchDataY');
        end

        % target network sunc
        if mod(t, H_target) == 0
            net_target = net;
        end
    end

    % ===== "centroid" loop: update Y given current P and stochasticity =====
    
    Theta_ij = cell(K,1); Par_memory = cell(K,1); 
    X_par = cell(K,1); Y_par = cell(K,1); net_Y = cell(K,1);
    for k = 1 : K
        Theta_ij{k} = theta_target; X_par{k} = X; Y_par{k} = Y;
        Par_memory{k} = memory; net_Y{k} = net;
    end
    
    parfor l = 1 : K
        for t = 1 : MaxInnerY
            idx = randi([1 buf_n]);
            i = Par_memory{l}.i(idx); j = Par_memory{l}.j(idx); k = Par_memory{l}.k(idx);
            d_hat = zeros(1,K);
            for j1 = 1 : K
                d_hat(j1) = net{l}([X_par{l}(i,:), Y_par{l}(j1,:)]);
            end
            Pij = softmax_row(-(1/T)*d_hat);
            if k == l
                Y(l,:) = Y(l,:) - alpha*Pij(j)*(Y(l,:) - X_par{l}(i,:));
            end
        end
    end
    disp(T);
    T = tau*T;
end

%% functions required in the above script

function s = softmax_row(z)
    z = z - max(z,[],2);
    e = exp(z);
    s = e/sum(e);
end

function Y = kpp_init(X,K)
    % k-means++ seeding (2D version)
    M = size(X,1);
    Y = zeros(K,2);
    Y(1,:) = X(randi(M),:);
    D = sum((X - Y(1,:)).^2,2);
    for k = 2:K
        probs = D / sum(D);
        idx = randsample(M,1,true,probs);
        Y(k,:) = X(idx,:);
        D = min(D, sum((X - Y(k,:)).^2,2));
    end
end