{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25e670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "Using device: cuda\n",
      "[Seed fixed to 0]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import numpy as np\n",
    "import torch\n",
    "from ADEN import ADEN\n",
    "from torchinfo import summary\n",
    "from TestCaseGenerator import data_RLClustering\n",
    "from ADENTrain import TrainAnneal\n",
    "import utils\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "utils.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c722a",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b89cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR SYNTHETIC DATA\n",
    "# X, M, T_P, N, d = data_RLClustering(4)\n",
    "# X = torch.tensor(X).float().to(device)\n",
    "# Y = torch.mean(X, dim=0, keepdim=True).to(device) + 0.01 * torch.randn(M, d).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aba9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DECENTRALIZED SENSING DATA\n",
    "import scipy\n",
    "address = f\"MATLAB Codes/UTD19_London.mat\"\n",
    "# read as numpy array\n",
    "data = scipy.io.loadmat(address)\n",
    "locs = data['Xz']\n",
    "# normalize locs to be in [0,1]x[0,1]\n",
    "locs = (locs - np.min(locs, axis=0)) / (np.max(locs, axis=0) - np.min(locs, axis=0))\n",
    "X = torch.tensor(locs).float().to(device)\n",
    "N, d = X.shape\n",
    "M = 100\n",
    "Y = torch.mean(X, dim=0, keepdim=True).to(device) + 0.01 * torch.randn(M, d).to(device)\n",
    "T_P = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfba11",
   "metadata": {},
   "source": [
    "# Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4625f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0\n",
    "gamma = 0.0\n",
    "zeta = 1.0\n",
    "T = 0.01\n",
    "parametrized = True\n",
    "from Env import ClusteringEnvTorch\n",
    "\n",
    "env = ClusteringEnvTorch(\n",
    "    n_data=N,\n",
    "    n_clusters=M,\n",
    "    n_features=d,\n",
    "    parametrized=parametrized,\n",
    "    eps=eps,\n",
    "    gamma=gamma,\n",
    "    zeta=zeta,\n",
    "    T=T,\n",
    "    T_p=torch.tensor(T_P),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a530fa",
   "metadata": {},
   "source": [
    "# Loading ADEN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a4af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "ADEN                                               --\n",
      "├─Linear: 1-1                                      192\n",
      "├─Linear: 1-2                                      192\n",
      "├─ModuleList: 1-3                                  --\n",
      "│    └─AdaptiveDistanceBlock: 2-1                  --\n",
      "│    │    └─MultiHeadDistanceAttention: 3-1        16,448\n",
      "│    │    └─LayerNorm: 3-2                         128\n",
      "│    │    └─LayerNorm: 3-3                         128\n",
      "│    │    └─Sequential: 3-4                        16,576\n",
      "│    │    └─Dropout: 3-5                           --\n",
      "│    └─AdaptiveDistanceBlock: 2-2                  --\n",
      "│    │    └─MultiHeadDistanceAttention: 3-6        16,448\n",
      "│    │    └─LayerNorm: 3-7                         128\n",
      "│    │    └─LayerNorm: 3-8                         128\n",
      "│    │    └─Sequential: 3-9                        16,576\n",
      "│    │    └─Dropout: 3-10                          --\n",
      "│    └─AdaptiveDistanceBlock: 2-3                  --\n",
      "│    │    └─MultiHeadDistanceAttention: 3-11       16,448\n",
      "│    │    └─LayerNorm: 3-12                        128\n",
      "│    │    └─LayerNorm: 3-13                        128\n",
      "│    │    └─Sequential: 3-14                       16,576\n",
      "│    │    └─Dropout: 3-15                          --\n",
      "│    └─AdaptiveDistanceBlock: 2-4                  --\n",
      "│    │    └─MultiHeadDistanceAttention: 3-16       16,448\n",
      "│    │    └─LayerNorm: 3-17                        128\n",
      "│    │    └─LayerNorm: 3-18                        128\n",
      "│    │    └─Sequential: 3-19                       16,576\n",
      "│    │    └─Dropout: 3-20                          --\n",
      "├─LayerNorm: 1-4                                   128\n",
      "├─Sequential: 1-5                                  --\n",
      "│    └─Linear: 2-5                                 8,256\n",
      "│    └─GELU: 2-6                                   --\n",
      "│    └─Dropout: 2-7                                --\n",
      "│    └─Linear: 2-8                                 2,080\n",
      "│    └─GELU: 2-9                                   --\n",
      "│    └─Dropout: 2-10                               --\n",
      "│    └─Linear: 2-11                                33\n",
      "===========================================================================\n",
      "Total params: 144,001\n",
      "Trainable params: 144,001\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_DIM = d  # dimensionality of the input space\n",
    "D_MODEL = 64  # dimensionality of the model\n",
    "N_LAYERS = 4  # number of layers\n",
    "N_HEADS = 8  # number of attention heads\n",
    "D_FF = 128  # dimensionality of the feedforward network\n",
    "DROPOUT = 0.05  # dropout rate\n",
    "\n",
    "model = ADEN(\n",
    "    input_dim=INPUT_DIM,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ca1dc",
   "metadata": {},
   "source": [
    "# Main Annealing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13943ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Annealing step: Beta = 1.000e+01 ===\n",
      "[TrainDbar_Hybrid_vec] Epoch 0, MSE Loss: 3.080e+01\n",
      "[TrainDbar_Hybrid_vec] Epoch 1000, MSE Loss: 1.248e+00\n",
      "[trainY] Epoch 0, F: -3.669e-01\n",
      "[trainY] Converged at epoch 1, F: -3.669e-01\n",
      "\n",
      "=== Annealing step: Beta = 1.100e+01 ===\n",
      "[TrainDbar_Hybrid_vec] Epoch 0, MSE Loss: 1.468e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m BETA_GROWTH_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.1\u001b[39m \n\u001b[0;32m     19\u001b[0m PERTURBATION_STD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m \n\u001b[1;32m---> 22\u001b[0m Y_opt, pi, history_y_all, history_pi_all , Betas \u001b[38;5;241m=\u001b[39m \u001b[43mTrainAnneal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TrainDbar hyperparameters\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples_in_batch_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SAMPLES_IN_BATCH_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHT_DECAY_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol_train_dbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOL_TRAIN_DBAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# trainY hyperparameters\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_train_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_train_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_train_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_train_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHT_DECAY_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol_train_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOL_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# annealing schedule\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBETA_INIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_final\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBETA_F\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_growth_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBETA_GROWTH_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperturbation_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPERTURBATION_STD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salar\\OneDrive\\Documents\\GitHub\\RL_Clustering\\ReinforcementBasedClustering\\ADENTrain.py:854\u001b[0m, in \u001b[0;36mTrainAnneal\u001b[1;34m(model, X, Y, env, device, epochs_dbar, batch_size_dbar, num_samples_in_batch_dbar, lr_dbar, weight_decay_dbar, tol_train_dbar, epochs_train_y, batch_size_train_y, lr_train_y, weight_decay_train_y, tol_train_y, beta_init, beta_final, beta_growth_rate, perturbation_std)\u001b[0m\n\u001b[0;32m    852\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (beta \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# --- TrainDbar ---\u001b[39;00m\n\u001b[1;32m--> 854\u001b[0m \u001b[43mTrainDbar_Hybrid_vec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples_in_batch_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol_train_dbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperturbation_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbation_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# --- trainY ---\u001b[39;00m\n\u001b[0;32m    872\u001b[0m Y, history_y \u001b[38;5;241m=\u001b[39m trainY(\n\u001b[0;32m    873\u001b[0m     model,\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    883\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    884\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\salar\\OneDrive\\Documents\\GitHub\\RL_Clustering\\ReinforcementBasedClustering\\ADENTrain.py:626\u001b[0m, in \u001b[0;36mTrainDbar_Hybrid_vec\u001b[1;34m(model, X, Y, env, device, epochs, batch_size, num_samples_in_batch, lr, weight_decay, tol, alpha, mc_samples, perturbation_std, epsilon, verbose)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;66;03m# --- Vectorized multinomial sampling / transition probs ---\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 626\u001b[0m     realized_clusters \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_indices_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmc_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, mc)\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# gather centroids for all MC samples:\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# prepare realized_Y template shape (B, M, mc, dim)\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     realized_Y_template \u001b[38;5;241m=\u001b[39m Y_batches\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, M, mc_samples, input_dim)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salar\\OneDrive\\Documents\\GitHub\\RL_Clustering\\ReinforcementBasedClustering\\Env.py:269\u001b[0m, in \u001b[0;36mClusteringEnvTorch.step\u001b[1;34m(self, batch_indices_all, idx, B, S, mc_samples, X, Y)\u001b[0m\n\u001b[0;32m    267\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparametrized:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update self.prob if needed\u001b[39;00m\n\u001b[0;32m    270\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    272\u001b[0m m_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(M, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, M)\u001b[38;5;241m.\u001b[39mexpand(B, S, M)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salar\\OneDrive\\Documents\\GitHub\\RL_Clustering\\ReinforcementBasedClustering\\Env.py:221\u001b[0m, in \u001b[0;36mClusteringEnvTorch.return_probabilities\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    217\u001b[0m u_masked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m    218\u001b[0m     mask, u, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m u_masked_mins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(u_masked, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 221\u001b[0m exp_u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mu_masked\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_masked_mins\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_u\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (1, j, i)\u001b[39;00m\n\u001b[0;32m    224\u001b[0m prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m exp_u \u001b[38;5;241m/\u001b[39m denom, torch\u001b[38;5;241m.\u001b[39mzeros_like(u))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# writing all hyperparameters in one place\n",
    "EPOCHS_DBAR = 1000\n",
    "BATCH_SIZE_DBAR = 32\n",
    "NUM_SAMPLES_IN_BATCH_DBAR = 128\n",
    "LR_DBAR = 1e-4\n",
    "WEIGHT_DECAY_DBAR = 1e-5\n",
    "TOL_TRAIN_DBAR = 1e-6\n",
    "PROBS_DBAR = torch.tensor(T_P)\n",
    "\n",
    "EPOCHS_TRAIN_Y = 100 \n",
    "BATCH_SIZE_TRAIN_Y = None\n",
    "LR_TRAIN_Y = 1e-4 \n",
    "WEIGHT_DECAY_TRAIN_Y = 1e-5\n",
    "TOL_TRAIN_Y = 1e-4\n",
    "\n",
    "BETA_INIT = 10.0\n",
    "BETA_F = 10000.0\n",
    "BETA_GROWTH_RATE = 1.1 \n",
    "PERTURBATION_STD = 0.01 \n",
    "\n",
    "\n",
    "Y_opt, pi, history_y_all, history_pi_all , Betas = TrainAnneal(\n",
    "    model,\n",
    "    X,\n",
    "    Y,\n",
    "    env,\n",
    "    device,\n",
    "    # TrainDbar hyperparameters\n",
    "    epochs_dbar=EPOCHS_DBAR,\n",
    "    batch_size_dbar=BATCH_SIZE_DBAR,\n",
    "    num_samples_in_batch_dbar=NUM_SAMPLES_IN_BATCH_DBAR,\n",
    "    lr_dbar=LR_DBAR,\n",
    "    weight_decay_dbar=WEIGHT_DECAY_DBAR,\n",
    "    tol_train_dbar=TOL_TRAIN_DBAR,\n",
    "    # trainY hyperparameters\n",
    "    epochs_train_y=EPOCHS_TRAIN_Y,\n",
    "    batch_size_train_y=BATCH_SIZE_TRAIN_Y,\n",
    "    lr_train_y=LR_TRAIN_Y,\n",
    "    weight_decay_train_y=WEIGHT_DECAY_TRAIN_Y,\n",
    "    tol_train_y=TOL_TRAIN_Y,\n",
    "    # annealing schedule\n",
    "    beta_init=BETA_INIT,\n",
    "    beta_final=BETA_F,\n",
    "    beta_growth_rate=BETA_GROWTH_RATE,\n",
    "    perturbation_std=PERTURBATION_STD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Plotter import PlotClustering\n",
    "\n",
    "\n",
    "X_np = X.detach().cpu().numpy()\n",
    "y_np = Y_opt.detach().cpu().numpy()\n",
    "\n",
    "PlotClustering(X_np, y_np, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76651739",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_name = \"Alg5_idx4_75\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5bcf4",
   "metadata": {},
   "source": [
    "# Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "\n",
    "\n",
    "# save_dict = {\n",
    "#     \"N\": N,\n",
    "#     \"M\": M,\n",
    "#     \"d\": d,\n",
    "#     # Hyperparameters\n",
    "#     \"EPOCHS_DBAR\": EPOCHS_DBAR,\n",
    "#     \"BATCH_SIZE_DBAR\": BATCH_SIZE_DBAR,\n",
    "#     \"NUM_SAMPLES_IN_BATCH_DBAR\": NUM_SAMPLES_IN_BATCH_DBAR,\n",
    "#     \"LR_DBAR\": LR_DBAR,\n",
    "#     \"WEIGHT_DECAY_DBAR\": WEIGHT_DECAY_DBAR,\n",
    "#     \"TOL_TRAIN_DBAR\": TOL_TRAIN_DBAR,\n",
    "#     \"GAMMA_DBAR\": GAMMA_DBAR,\n",
    "#     \"PROBS_DBAR\": PROBS_DBAR,\n",
    "#     \"EPOCHS_TRAIN_Y\": EPOCHS_TRAIN_Y,\n",
    "#     \"BATCH_SIZE_TRAIN_Y\": BATCH_SIZE_TRAIN_Y,\n",
    "#     \"LR_TRAIN_Y\": LR_TRAIN_Y,\n",
    "#     \"WEIGHT_DECAY_TRAIN_Y\": WEIGHT_DECAY_TRAIN_Y,\n",
    "#     \"TOL_TRAIN_Y\": TOL_TRAIN_Y,\n",
    "#     \"BETA_INIT\": BETA_INIT,\n",
    "#     \"BETA_F\": BETA_F,\n",
    "#     \"BETA_GROWTH_RATE\": BETA_GROWTH_RATE,\n",
    "#     \"PERTURBATION_STD\": PERTURBATION_STD,\n",
    "#     # Results\n",
    "#     \"X\": X,\n",
    "#     \"Y_opt\": Y_opt,\n",
    "#     \"history_y_all\": history_y_all,\n",
    "#     \"history_pi_all\": history_pi_all,\n",
    "#     # Model architecture\n",
    "#     \"INPUT_DIM\": INPUT_DIM,\n",
    "#     \"D_MODEL\": D_MODEL,\n",
    "#     \"N_LAYERS\": N_LAYERS,\n",
    "#     \"N_HEADS\": N_HEADS,\n",
    "#     \"D_FF\": D_FF,\n",
    "#     \"DROPOUT\": DROPOUT\n",
    "# }\n",
    "\n",
    "# with open(f\"Results/{scenario_name}_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(save_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5a31e",
   "metadata": {},
   "source": [
    "# Load the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Now craete a load function to load the results\n",
    "# def load_results(scenario_name):\n",
    "#     with open(f\"Results/{scenario_name}_results.pkl\", \"rb\") as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data\n",
    "# results = load_results(scenario_name)\n",
    "# X = results[\"X\"]\n",
    "# Y_opt = results[\"Y_opt\"]\n",
    "# history_y_all = results[\"history_y_all\"]\n",
    "# history_pi_all = results[\"history_pi_all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13678adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animator import animate_Clusters\n",
    "# history_pi_all[0] = history_pi_all[1]\n",
    "animate_Clusters(\n",
    "    X.detach().cpu().numpy(),\n",
    "    history_y_all,\n",
    "    history_pi_all,\n",
    "    Betas,\n",
    "    interval=500,\n",
    "    save_path=f\"Results/Deep_animation.gif\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
